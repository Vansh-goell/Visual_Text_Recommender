{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80910\n",
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n"
     ]
    }
   ],
   "source": [
    "df  = pd.read_csv('flickr8k/captions.txt')\n",
    "print(df.size)\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see for each image there are 5 text lines 1->a  1->b  .. 1->e   let group them \n",
    "#now can groupy\n",
    "image_captions = df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n",
    "# df.groupby(\"image\") – groups the DataFrame rows by the \"image\" column\n",
    "# [\"caption\"] – selects the \"caption\" column within each group.\n",
    "# .apply(list) – converts all captions in each group into a list.\n",
    "# .to_dict() – converts the grouped result into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n",
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n",
      "10815824_2997e03d76.jpg\n",
      "['A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel .', 'A girl and her horse stand by a fire .', \"A girl holding a horse 's lead behind a fire .\", 'A man , and girl and two horses are near a contained fire .', 'Two people and two horses watching a fire .']\n"
     ]
    }
   ],
   "source": [
    "first_key = list(image_captions.keys())[0]\n",
    "print(first_key)\n",
    "print(image_captions[first_key])\n",
    "print('10815824_2997e03d76.jpg')\n",
    "print(image_captions['10815824_2997e03d76.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# \"\"\"\n",
    "#  Summary\n",
    "# Feature\tdict\tdefaultdict\n",
    "# Handles missing keys\t❌ No (gives error)\t✅ Yes (auto-creates default)\n",
    "# Needs key check\t✅ Yes\t❌ No\n",
    "# Cleaner code\t❌ Longer\t✅ Shorter\n",
    "# Use when\tYou want full control\tYou want convenience\n",
    "# \"\"\"\n",
    "import nltk\n",
    "import os\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_keywords = defaultdict(set)  #it is a dict having key as image 1.jpg and value as set of noun words\n",
    "# Creates an empty defaultdict where each value is a set.\n",
    "# 💡 Why: We’ll store, for each image, the set of all nouns found in its captions.\n",
    "# ✨ Benefit: No need to check if the key exists before adding to it.\n",
    "\n",
    "for image, captions in image_captions.items():\n",
    "    for caption in captions:\n",
    "        words = word_tokenize(caption.lower())\n",
    "        nouns = [word for word, tag in pos_tag(words) if tag.startswith('NN')]\n",
    "        # nouns = []    or this for loop\n",
    "        # for word, tag in pos_tag(words):\n",
    "            #if tag.startswith('NN'):\n",
    "                #     nouns.append(word)\n",
    "        image_keywords[image].update(nouns)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for 1000268201_693b08cb0e.jpg : {'dress', 'child', 'set', 'entry', 'cabin', 'building', 'way', 'girl', 'pink', 'stairs', 'playhouse'}\n"
     ]
    }
   ],
   "source": [
    "first_key_image = list(image_captions.keys())[0]\n",
    "print(\"Keywords for\", first_key_image, \":\", image_keywords[first_key_image])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do like any got keywords same >0 then simi else disimi\n",
    "import random # pick a random positive and a random negative example for each anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets created: 8091\n",
      "Example triplet: ('1000268201_693b08cb0e.jpg', '3308997740_91765ecdcc.jpg', '2461631708_decc5b8c87.jpg')\n"
     ]
    }
   ],
   "source": [
    "#now will iterate over all image get its keyworks from image_captions to compare \n",
    "triplets = []\n",
    "image_list = list(image_keywords.keys())\n",
    "#now iterate and take respective keywords for each image\n",
    "for anchor in image_list:\n",
    "    anchor_keywords = image_keywords[anchor]\n",
    "    \n",
    "    #now to find positive at least one same keyword\n",
    "    positives = [img for img in image_list if img!=anchor and  len(image_keywords[img] & anchor_keywords)>0]\n",
    "    negatives = [img for img in image_list if img!=anchor and len(image_keywords[img] & anchor_keywords)==0]\n",
    "    \n",
    "    if positives and negatives:\n",
    "        pos = random.choice(positives)\n",
    "        neg = random.choice(negatives)\n",
    "        triplets.append((anchor,pos,neg))\n",
    "\n",
    "print(\"Total triplets created:\", len(triplets))\n",
    "print(\"Example triplet:\", triplets[0])\n",
    "\n",
    "#see now i caan access keywords also by these keys(image name in triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Build tf.data.Dataset of Raw Paths + Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_paths_and_captions(triplets, image_captions):\n",
    "    def generator():\n",
    "        for a, p, n in triplets:\n",
    "            yield (\n",
    "                f\"flickr8k/images/{a}\", \" \".join(image_keywords[a]),   # Anchor keywords as text\n",
    "                f\"flickr8k/images/{p}\", \" \".join(image_keywords[p]),   # Positive\n",
    "                f\"flickr8k/images/{n}\", \" \".join(image_keywords[n]),   # Negative\n",
    "            )\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec([], tf.string),  # anchor image path\n",
    "            tf.TensorSpec([], tf.string),  # anchor keyword text\n",
    "            tf.TensorSpec([], tf.string),  # pos image path\n",
    "            tf.TensorSpec([], tf.string),  # pos keyword text\n",
    "            tf.TensorSpec([], tf.string),  # neg image path\n",
    "            tf.TensorSpec([], tf.string),  # neg keyword text\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define Multimodal Model + Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path, size=(224, 224)):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, size)\n",
    "    return tf.keras.applications.resnet50.preprocess_input(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format & Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_triplet(a_path, a_txt, p_path, p_txt, n_path, n_txt):\n",
    "    return (\n",
    "        preprocess_image(a_path), a_txt,\n",
    "        preprocess_image(p_path), p_txt,\n",
    "        preprocess_image(n_path), n_txt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(triplets, image_captions, batch_size=16):\n",
    "    raw_ds = get_triplet_paths_and_captions(triplets, image_captions)\n",
    "    ds = (\n",
    "        raw_ds\n",
    "        .map(preprocess_triplet, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .shuffle(1000)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    )\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_image_branch():\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False, weights=\"imagenet\", pooling=\"avg\", input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze ResNet\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    x = base_model(inputs)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    return tf.keras.Model(inputs, x, name=\"ImageBranch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "def build_text_branch():\n",
    "    use = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False)\n",
    "    inputs = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "    x = use(inputs)\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    return tf.keras.Model(inputs, x, name=\"TextBranch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fusion_model():\n",
    "    inputs = tf.keras.Input(shape=(512,))  # 256 img + 256 text\n",
    "    x = tf.keras.layers.Dense(128)(inputs)\n",
    "    x = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1))(x)\n",
    "    return tf.keras.Model(inputs, x, name=\"FusionHead\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor, positive, negative, margin=0.3):\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=1)\n",
    "    loss = tf.maximum(pos_dist - neg_dist + margin, 0.0)\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:41:57.580850: W tensorflow/core/framework/op_kernel.cc:1833] OP_REQUIRES failed at cast_op.cc:122 : UNIMPLEMENTED: Cast string to float is not supported\n",
      "2025-07-06 16:41:57.582250: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1m{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported [Op:Cast] name: \u001b[0m\n\nArguments received by Functional.call():\n  • inputs=tf.Tensor(shape=(16,), dtype=string)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Step 1: Get image embeddings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     anchor_img_emb \u001b[38;5;241m=\u001b[39m image_model(anchor_img, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m     pos_img_emb    \u001b[38;5;241m=\u001b[39m image_model(pos_img, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m     neg_img_emb    \u001b[38;5;241m=\u001b[39m image_model(neg_img, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Step 2: Get text embeddings\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6006\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6005\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6006\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1m{{function_node __wrapped__Cast_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cast string to float is not supported [Op:Cast] name: \u001b[0m\n\nArguments received by Functional.call():\n  • inputs=tf.Tensor(shape=(16,), dtype=string)\n  • training=True\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# 1. Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# 2. Training Loop\n",
    "EPOCHS = 5  # Set to higher later\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in ds:\n",
    "        anchor_img, pos_img, neg_img, anchor_text, pos_text, neg_text = batch\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Step 1: Get image embeddings\n",
    "            anchor_img_emb = image_model(anchor_img, training=True)\n",
    "            pos_img_emb    = image_model(pos_img, training=True)\n",
    "            neg_img_emb    = image_model(neg_img, training=True)\n",
    "\n",
    "            # Step 2: Get text embeddings\n",
    "            anchor_txt_emb = text_model(anchor_text, training=True)\n",
    "            pos_txt_emb    = text_model(pos_text, training=True)\n",
    "            neg_txt_emb    = text_model(neg_text, training=True)\n",
    "\n",
    "            # Step 3: Fuse image + text\n",
    "            a_fused = fusion_model(tf.concat([anchor_img_emb, anchor_txt_emb], axis=1), training=True)\n",
    "            p_fused = fusion_model(tf.concat([pos_img_emb, pos_txt_emb], axis=1), training=True)\n",
    "            n_fused = fusion_model(tf.concat([neg_img_emb, neg_txt_emb], axis=1), training=True)\n",
    "\n",
    "            # Step 4: Compute loss\n",
    "            loss = triplet_loss(a_fused, p_fused, n_fused)\n",
    "\n",
    "        # Step 5: Backpropagation\n",
    "        grads = tape.gradient(loss,\n",
    "                              image_model.trainable_weights +\n",
    "                              text_model.trainable_weights +\n",
    "                              fusion_model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads,\n",
    "                                      image_model.trainable_weights +\n",
    "                                      text_model.trainable_weights +\n",
    "                                      fusion_model.trainable_weights))\n",
    "\n",
    "        total_loss += loss.numpy()\n",
    "        num_batches += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each model to use later\n",
    "image_model.save(\"image_model.h5\")\n",
    "text_model.save(\"text_model.h5\")\n",
    "fusion_model.save(\"fusion_model.h5\")\n",
    "print(\"Models saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "image_model = load_model(\"image_model.h5\")\n",
    "text_model  = load_model(\"text_model.h5\")\n",
    "fusion_model = load_model(\"fusion_model.h5\")\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_embedding(image_path, caption):\n",
    "    # Image preprocessing\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    img = tf.expand_dims(img, axis=0)  # shape: (1, 224, 224, 3)\n",
    "\n",
    "    img_emb = image_model.predict(img)\n",
    "    txt_emb = text_model.predict([caption])  # shape: (1, 256)\n",
    "\n",
    "    fused_input = tf.concat([img_emb, txt_emb], axis=1)  # shape: (1, 512)\n",
    "    fused_emb = fusion_model(fused_input)  # shape: (1, 128)\n",
    "    return fused_emb.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_images(query_image, query_caption, image_caption_dict, top_k=5):\n",
    "    query_emb = get_combined_embedding(query_image, query_caption)\n",
    "\n",
    "    results = []\n",
    "    for img_path, caption in image_caption_dict.items():\n",
    "        emb = get_combined_embedding(img_path, caption)\n",
    "        score = cosine_similarity([query_emb], [emb])[0][0]\n",
    "        results.append((img_path, score))\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query_path = \"flickr8k/images/123456.jpg\"\n",
    "query_caption = \"A child is playing with a ball\"\n",
    "\n",
    "top_results = recommend_images(query_path, query_caption, image_captions, top_k=5)\n",
    "\n",
    "for img, score in top_results:\n",
    "    print(f\"{img} | Score: {score:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
