{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUR Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('flickr8k/captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80910\n"
     ]
    }
   ],
   "source": [
    "print(df.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see for each image there are 5 text lines 1->a  1->b  .. 1->e   let group them \n",
    "#now can groupy\n",
    "image_captions = df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n",
    "# df.groupby(\"image\") – groups the DataFrame rows by the \"image\" column\n",
    "# [\"caption\"] – selects the \"caption\" column within each group.\n",
    "# .apply(list) – converts all captions in each group into a list.\n",
    "# .to_dict() – converts the grouped result into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n",
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n",
      "10815824_2997e03d76.jpg\n",
      "['A blonde horse and a blonde girl in a black sweatshirt are staring at a fire in a barrel .', 'A girl and her horse stand by a fire .', \"A girl holding a horse 's lead behind a fire .\", 'A man , and girl and two horses are near a contained fire .', 'Two people and two horses watching a fire .']\n"
     ]
    }
   ],
   "source": [
    "first_key = list(image_captions.keys())[0]\n",
    "print(first_key)\n",
    "print(image_captions[first_key])\n",
    "print('10815824_2997e03d76.jpg')\n",
    "print(image_captions['10815824_2997e03d76.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary or Keyword Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# \"\"\"\n",
    "#  Summary\n",
    "# Feature\tdict\tdefaultdict\n",
    "# Handles missing keys\t❌ No (gives error)\t✅ Yes (auto-creates default)\n",
    "# Needs key check\t✅ Yes\t❌ No\n",
    "# Cleaner code\t❌ Longer\t✅ Shorter\n",
    "# Use when\tYou want full control\tYou want convenience\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt', download_dir='/usr/local/share/nltk_data')\n",
    " #provides: A pre-trained tokenizer model that knows how to break English sentences into words and punctuation.\n",
    "# nltk.download('averaged_perceptron_tagger') # provides: A model that labels each word with its part-of-speech (noun, verb, adjective, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.data.path = [os.path.expanduser(\"~/nltk_data\")]  # ✅ must come before pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_keywords = defaultdict(set)  #it is a dict having key as image 1.jpg and value as set of noun words\n",
    "# Creates an empty defaultdict where each value is a set.\n",
    "# 💡 Why: We’ll store, for each image, the set of all nouns found in its captions.\n",
    "# ✨ Benefit: No need to check if the key exists before adding to it.\n",
    "\n",
    "for image, captions in image_captions.items():\n",
    "    for caption in captions:\n",
    "        words = word_tokenize(caption.lower())\n",
    "        nouns = [word for word, tag in pos_tag(words) if tag.startswith('NN')]\n",
    "        # nouns = []    or this for loop\n",
    "        # for word, tag in pos_tag(words):\n",
    "            #if tag.startswith('NN'):\n",
    "                #     nouns.append(word)\n",
    "        image_keywords[image].update(nouns)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for 1000268201_693b08cb0e.jpg : {'playhouse', 'girl', 'building', 'cabin', 'dress', 'stairs', 'way', 'entry', 'child', 'set', 'pink'}\n"
     ]
    }
   ],
   "source": [
    "first_key_image = list(image_captions.keys())[0]\n",
    "print(\"Keywords for\", first_key_image, \":\", image_keywords[first_key_image])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making Triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do like any got keywords same >0 then simi else disimi\n",
    "import random # pick a random positive and a random negative example for each anchor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triplets created: 8091\n",
      "Example triplet: ('1000268201_693b08cb0e.jpg', '3767841911_6678052eb6.jpg', '530454257_66d58b49ee.jpg')\n"
     ]
    }
   ],
   "source": [
    "#now will iterate over all image get its keyworks from image_captions to compare \n",
    "triplets = []\n",
    "image_list = list(image_keywords.keys())\n",
    "#now iterate and take respective keywords for each image\n",
    "for anchor in image_list:\n",
    "    anchor_keywords = image_keywords[anchor]\n",
    "    \n",
    "    #now to find positive at least one same keyword\n",
    "    positives = [img for img in image_list if img!=anchor and  len(image_keywords[img] & anchor_keywords)>0]\n",
    "    negatives = [img for img in image_list if img!=anchor and len(image_keywords[img] & anchor_keywords)==0]\n",
    "    \n",
    "    if positives and negatives:\n",
    "        pos = random.choice(positives)\n",
    "        neg = random.choice(negatives)\n",
    "        triplets.append((anchor,pos,neg))\n",
    "\n",
    "print(\"Total triplets created:\", len(triplets))\n",
    "print(\"Example triplet:\", triplets[0])\n",
    "\n",
    "#see now i caan access keywords also by these keys(image name in triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is imp . as see in above for one anchor 1 triplet , in this if po==5  ne =8 i can make make many diff with unique anchor , more diverse data but size more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# triplets = []\n",
    "# image_list = list(image_keywords.keys())\n",
    "\n",
    "# # Example “combos” of (pos_count, neg_count) you’d like to try:\n",
    "# combos = [(1, 3),   # pick 1 positive & 3 negatives\n",
    "#           (5, 6)]   # pick 5 positives & 6 negatives\n",
    "\n",
    "# for anchor in image_list:\n",
    "#     anchor_keywords = image_keywords[anchor]\n",
    "    \n",
    "#     # Build your full candidate lists once per anchor\n",
    "#     positives = [\n",
    "#         img for img in image_list\n",
    "#         if img != anchor\n",
    "#            and len(image_keywords[img] & anchor_keywords) > 0\n",
    "#     ]\n",
    "#     negatives = [\n",
    "#         img for img in image_list\n",
    "#         if len(image_keywords[img] & anchor_keywords) == 0\n",
    "#     ]\n",
    "    \n",
    "#     # Try each (pos_no, neg_no) pair\n",
    "#     for pos_no, neg_no in combos:\n",
    "#         # Ensure you actually have enough candidates\n",
    "#         if pos_no <= len(positives) and neg_no <= len(negatives):\n",
    "#             # Randomly pick pos_no distinct positives\n",
    "#             pos_samples = random.sample(positives, pos_no)    \n",
    "#             # Randomly pick neg_no distinct negatives\n",
    "#             neg_samples = random.sample(negatives, neg_no)    \n",
    "\n",
    "#             # Now form every combination of those picks\n",
    "#             for pos in pos_samples:            # pos is a str (filename)\n",
    "#                 for neg in neg_samples:        # neg is a str (filename)\n",
    "#                     triplets.append((anchor, pos, neg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now code till training model [img emb , text emb , fuse , get emb fn , triplet loss , training loop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow-hub in /opt/anaconda3/lib/python3.12/site-packages (0.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow-hub) (2.19.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "#see need to get models\n",
    "!pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model defifnatin\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers,Model\n",
    "\n",
    "# Model\n",
    "# ├── 📷 Image Branch\n",
    "# │   ├── ResNet50 (frozen)\n",
    "# │   │   └── Output: 2048-d\n",
    "# │   └── img_proj (Dense 256 + ReLU)\n",
    "# │       └── Output: 256-d\n",
    "# │\n",
    "# ├── 📝 Text Branch\n",
    "# │   ├── Universal Sentence Encoder (USE)\n",
    "# │   │   └── Output: 512-d\n",
    "# │   └── txt_proj (Dense 256 + ReLU)\n",
    "# │       └── Output: 256-d\n",
    "# │\n",
    "# ├── 🔗 Fusion\n",
    "# │   ├── Concatenate (img_proj + txt_proj) → 512-d\n",
    "# │   └── fusion_head\n",
    "# │       ├── Dense(128)\n",
    "# │       └── L2 Normalize\n",
    "# │           └── Output: 128-d embedding (final joint embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load pretrained model\n",
    "\n",
    "resnet = tf.keras.applications.ResNet50(include_top=False, pooling='avg', input_shape=(224, 224, 3))\n",
    "resnet.trainable = False\n",
    "\n",
    "\n",
    "#for text\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "\n",
    "#adding our own layers dense layers at these models\n",
    "img_proj = tf.keras.Sequential([   #a small neural network over our resnet , its weight we will train to get best embeddings out\n",
    "    layers.Dense(256,activation = 'relu')\n",
    "])\n",
    "\n",
    "text_proj = tf.keras.Sequential([\n",
    "    layers.Dense(256, activation='relu')\n",
    "])\n",
    "\n",
    "fusion_head = tf.keras.Sequential([  #also a layer to train in which we pass concat(txt,img) emb  and get final emb  \n",
    "    layers.Dense(128), \n",
    "    layers.Lambda(lambda x: tf.math.l2_normalize(x,axis=1))   #this is my model to train\n",
    "])\n",
    "\n",
    "\n",
    "#3 layer to train --> img_proj  , text_proj , fusion_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Embedding\n",
    "\n",
    "def get_image_embedding(image_path,model = resnet,projector = img_proj): #projector is out new layer we are adding\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path,target_size = (224,224))\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    img = tf.expand_dims(img,axis=0)   #add batch here as we pass image in a batch to resent\n",
    "    features = model.predict(img)\n",
    "    projected = projector(features)\n",
    "    return projected.numpy()[0] # Remove batch, shape (256,)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text embedding\n",
    "\n",
    "def get_text_embedding(caption, model=use, projector=text_proj):\n",
    "    features = model([caption])  # shape: (1, 512)\n",
    "    projected = projector(features)      # shape: (1, 256)\n",
    "    return projected.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fused_embeddings(image_path,caption,model_img = resnet ,model_text=use,proj_img=img_proj, proj_txt=text_proj,fusion_model = fusion_head):\n",
    "    img_emb = get_image_embedding(image_path,model_img,proj_img)  #see we send model also as can happen we change model but then we don't need to remove resent and ue , as from here we are controllin gmodel we are sending , not using by default inthe get_imaage_emb\n",
    "    txt_emb = get_text_embedding(caption , model_text, proj_txt)\n",
    "\n",
    "    fused = tf.concat([img_emb, txt_emb], axis=0)  # ✅ axis=0 for 1D arrays        \n",
    "    output = fusion_model(tf.expand_dims(fused, axis=0))  # Add batch, then normalize\n",
    "    return output.numpy()[0]  # shape: (128,)\n",
    "\n",
    "#this is my model see get embeddings , fused them , but just didn't train like a,b  a,c distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_emb = []\n",
    "# for anchor, pos,neg in triplets:\n",
    "#     anchor_caption = image_captions[anchor][0]\n",
    "#     pos_caption = image_captions[pos][0]\n",
    "#     neg_caption = image_captions[neg][0]\n",
    "\n",
    "#     anchor_path =   f\"flickr8k/images/{anchor}\"           #this is image path flickr8k/Images/3637013_c675de7705.jpg\n",
    "#     pos_path = f\"flickr8k/images/{pos}\"\n",
    "#     neg_path = f\"flickr8k/images/{neg}\"\n",
    "\n",
    "#     anchor_emb = get_fused_embeddings(anchor_path, anchor_caption, resnet,use,img_proj,text_proj,fusion_head)\n",
    "#     pos_emb = get_fused_embeddings(pos_path,pos_caption,resnet,use,img_proj,text_proj,fusion_head)\n",
    "#     neg_emb =  get_fused_embeddings(neg_path,neg_caption,resnet,use,img_proj,text_proj,fusion_head)\n",
    "\n",
    "#     triplet_emb.append((anchor_emb, pos_emb, neg_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(triplet_emb,open('triplet_emb.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# triplet_emb_nouns = []\n",
    "\n",
    "# for anchor, pos, neg in tqdm(triplets):\n",
    "#     # 1️⃣ Build “caption” from noun keywords instead of real sentence\n",
    "#     anchor_caption = \" \".join(image_keywords[anchor])\n",
    "#     pos_caption    = \" \".join(image_keywords[pos])\n",
    "#     neg_caption    = \" \".join(image_keywords[neg])\n",
    "\n",
    "#     # 2️⃣ Build the image file paths\n",
    "#     anchor_path = f\"flickr8k/images/{anchor}\"\n",
    "#     pos_path    = f\"flickr8k/images/{pos}\"\n",
    "#     neg_path    = f\"flickr8k/images/{neg}\"\n",
    "\n",
    "#     # 3️⃣ Get fused embeddings using your existing function\n",
    "#     anchor_emb = get_fused_embeddings(anchor_path, anchor_caption, resnet,use,img_proj,text_proj,fusion_head)\n",
    "#     pos_emb    = get_fused_embeddings(pos_path, pos_caption, resnet,use,img_proj,text_proj,fusion_head)\n",
    "#     neg_emb    = get_fused_embeddings(neg_path, neg_caption, resnet,use,img_proj,text_proj,fusion_head)\n",
    "\n",
    "#     # 4️⃣ Append to new list\n",
    "#     triplet_emb_nouns.append((anchor_emb, pos_emb, neg_emb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(triplet_emb_nouns,open('triplet_emb_nouns.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load & Stack Precomputed Embeddings\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load your pickled triplet embeddings\n",
    "with open(\"triplet_emb.pkl\", \"rb\") as f:\n",
    "    triplet_emb = pickle.load(f)\n",
    "    # triplet_emb: list of tuples (anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "# Stack into three NumPy arrays of shape (N,128)\n",
    "anchors   = np.stack([t[0] for t in triplet_emb])  # shape: (N,128)    #see here using stack as it is efficient in case of batch ooperations\n",
    "positives = np.stack([t[1] for t in triplet_emb])  # shape: (N,128)\n",
    "negatives = np.stack([t[2] for t in triplet_emb])  # shape: (N,128)\n",
    "\n",
    "\n",
    "# | Feature              | `list`                    | `np.stack()`                  |\n",
    "# | -------------------- | ------------------------- | ----------------------------- |\n",
    "# | Type                 | Python `list` of arrays   | Single `numpy.ndarray`        |\n",
    "# | Shape                | N objects of shape (128,) | One array of shape (N, 128)   |\n",
    "# | Supports vector math | ❌ (must loop manually)    | ✅ Efficient & fast operations |\n",
    "# | TensorFlow friendly  | ❌ Hard to use             | ✅ Preferred for training      |\n",
    "# | Memory efficient     | ❌ More overhead           | ✅ Compact & optimized         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(anchors[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the tf.data.Dataset   ,see pipeline belwo and explaination\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Generator yields one triplet of embeddings at a time\n",
    "def triplet_generator():\n",
    "    for a, p, n in zip(anchors, positives, negatives):\n",
    "        yield a, p, n\n",
    "\n",
    "#  What it is: A little Python function that, each time you ask it, gives you the next (anchor, positive, negative) triple of embeddings.\n",
    "# Why: It’s a convenient way to stream your precomputed 128‑dim vectors one triplet at a time, without loading them all at once somewhere else.       \n",
    "\n",
    "# Create Dataset with correct output types/shapes\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "    triplet_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec((128,), tf.float32),   # Only when you call .batch(32) does TensorFlow accumulate 32 individual triplet elements into one batch.\n",
    "        tf.TensorSpec((128,), tf.float32),\n",
    "        tf.TensorSpec((128,), tf.float32),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# What it does:\n",
    "# Wraps your triplet_generator() in a TensorFlow object called a Dataset.\n",
    "# Tells TensorFlow, “Each item I yield is three float32 vectors of length 128.”\n",
    "# Why:\n",
    "# This turns your plain‑Python generator into something TensorFlow can optimize and plug directly into model.fit().\n",
    "# You get back a “stream” of (anchor, positive, negative) triplets as tf.Tensor objects, not NumPy arrays or Python lists.\n",
    "\n",
    "\n",
    "#in other dataset(Static) (Python list / NumPy array / Pandas DataFrame) we first load whole data then fn , in case of large data RAM full\n",
    "\n",
    "#in tf.data.Dataset(Streaming)     A tf.data.Dataset is TensorFlow’s way of representing and managing your data as a pipeline   A tf.data.Dataset is TensorFlow’s way of representing and managing your data as a pipeline of transformations and batches,(we creatae a pipeline , tell how to read the data we have ,  You define how to read, preprocess, and batch data.)  Data is fetched “just in time” in small pieces (streams) during training. ,  You never need to load the entire dataset into RAM.\n",
    "\n",
    "\n",
    "# Shuffle, batch, and prefetch for performance\n",
    "BATCH_SIZE = 32\n",
    "ds = (\n",
    "    ds\n",
    "    .shuffle(buffer_size=len(anchors))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "\n",
    "# What it does:\n",
    "\n",
    "\n",
    "# .shuffle(buffer_size=len(anchors)):->\n",
    "# Keeps a “buffer” of that many triplets and randomly picks the next one from within the buffer.\n",
    "# Why:\n",
    "# Prevents the model from seeing the same sequence of triplets every epoch, which helps it generalize better.\n",
    "\n",
    "\n",
    "\n",
    "# .batch(BATCH_SIZE):->\n",
    "# Rather than returning one triplet at a time, it groups BATCH_SIZE triplets into a single “batch.”\n",
    "# So instead of (anchor, pos, neg) shapes (128,), you now get three tensors of shape (BATCH_SIZE, 128).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# .prefetch(tf.data.AUTOTUNE):->\n",
    "# While your model is busy training on one batch, TensorFlow prepares (loads, shuffles, batches) the next batch in the background.\n",
    "\n",
    "\n",
    "# Composable Transformations\n",
    "# With a Dataset you can chain simple operations:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# ds = tf.data.Dataset.from_generator(...)\n",
    "#    .shuffle(buffer_size=1000)\n",
    "#    .map(preprocess_fn, num_parallel_calls=4)\n",
    "#    .batch(32)\n",
    "#    .prefetch(1)\n",
    "# Each of these methods returns a new Dataset:\n",
    "\n",
    "# shuffle: randomly reorders elements\n",
    "# map: applies a function (e.g. image decoding, augmentation)\n",
    "# batch: groups elements into fixed‑size batches\n",
    "# prefetch: overlaps data preparation with model execution\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
