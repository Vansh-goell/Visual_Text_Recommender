{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not trained took lot of ttime\n",
    "\n",
    "import os\n",
    "\n",
    "# ‚úÖ Enables CPU-optimized oneDNN ops (speeds up training)\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "# ‚úÖ Optional: Set visible threads (for reproducibility or performance tuning)\n",
    "os.environ['OMP_NUM_THREADS'] = '8'   # same as number of cores\n",
    "os.environ['KMP_BLOCKTIME'] = '0'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF_ENABLE_ONEDNN_OPTS: 1\n",
      "OMP_NUM_THREADS: 8\n",
      "KMP_BLOCKTIME: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"TF_ENABLE_ONEDNN_OPTS:\", os.environ.get('TF_ENABLE_ONEDNN_OPTS'))\n",
    "print(\"OMP_NUM_THREADS:\", os.environ.get('OMP_NUM_THREADS'))\n",
    "print(\"KMP_BLOCKTIME:\", os.environ.get('KMP_BLOCKTIME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threading set: Intra=8, Inter=2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# ‚úÖ Set CPU parallelism (you have 8 cores ‚Üí use 8 intra, 2 inter threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)\n",
    "\n",
    "print(\"Threading set: Intra=8, Inter=2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load captions and extract noun keywords\n",
    "def load_captions(path=\"flickr8k/captions.txt\"):\n",
    "    df = pd.read_csv(path)\n",
    "    return df.groupby(\"image\")[\"caption\"].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(image_captions):\n",
    "    kw = defaultdict(set)\n",
    "    for img, caps in image_captions.items():\n",
    "        for c in caps:\n",
    "            for w, tag in pos_tag(word_tokenize(c.lower())):\n",
    "                if tag.startswith(\"NN\"):\n",
    "                    kw[img].add(w)\n",
    "    return kw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captions = load_captions()\n",
    "image_keywords = extract_keywords(image_captions)\n",
    "keyword_sentences = {\n",
    "    img: \" \".join(sorted(words))\n",
    "    for img, words in image_keywords.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Generate triplets based on keyword overlap\n",
    "def generate_triplets(image_keywords):\n",
    "    triplets = []\n",
    "    images = list(image_keywords.keys())\n",
    "    for anchor in images:\n",
    "        kws = image_keywords[anchor]\n",
    "        pos = [i for i in images if i!=anchor and image_keywords[i]&kws]\n",
    "        neg = [i for i in images if i!=anchor and not (image_keywords[i]&kws)]\n",
    "        if pos and neg:\n",
    "            p = random.choice(pos)\n",
    "            n = random.choice(neg)\n",
    "            triplets.append((anchor,p,n))\n",
    "    return triplets\n",
    "\n",
    "triplets = generate_triplets(image_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Build TF dataset yielding images and keyword-based text\n",
    "def build_triplet_dataset(triplets, keyword_sentences, img_size=(224,224), batch_size=32):\n",
    "    def gen():\n",
    "        for a,p,n in triplets:\n",
    "            yield (\n",
    "                f\"flickr8k/images/{a}\", keyword_sentences[a],\n",
    "                f\"flickr8k/images/{p}\", keyword_sentences[p],\n",
    "                f\"flickr8k/images/{n}\", keyword_sentences[n],\n",
    "            )\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec((), tf.string), tf.TensorSpec((), tf.string),\n",
    "            tf.TensorSpec((), tf.string), tf.TensorSpec((), tf.string),\n",
    "            tf.TensorSpec((), tf.string), tf.TensorSpec((), tf.string),\n",
    "        )\n",
    "    )\n",
    "    def parse(a_img, a_txt, p_img, p_txt, n_img, n_txt):\n",
    "        def load_image(path):\n",
    "            img = tf.io.read_file(path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, img_size)/255.0\n",
    "            return img\n",
    "        return (\n",
    "            load_image(a_img), a_txt,\n",
    "            load_image(p_img), p_txt,\n",
    "            load_image(n_img), n_txt,\n",
    "        )\n",
    "    return ds.map(parse, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "             .shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds = build_triplet_dataset(triplets, keyword_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Build model branches\n",
    "def build_image_branch():\n",
    "    inp = Input(shape=(224,224,3), name=\"image_input\")\n",
    "    base = tf.keras.applications.ResNet50(include_top=False, pooling=\"avg\", weights=\"imagenet\")\n",
    "    base.trainable = False\n",
    "    x = base(inp)\n",
    "    x = layers.Dense(256, activation=\"relu\")(x)\n",
    "    return Model(inp, x, name=\"ImageBranch\")\n",
    "\n",
    "use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class UniversalEmbeddingLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "    def call(self, x):\n",
    "        x = tf.squeeze(tf.cast(x, tf.string))\n",
    "        return self.use(x)\n",
    "\n",
    "def build_text_branch():\n",
    "    inp = Input(shape=(1,), dtype=tf.string, name=\"text_input\")\n",
    "    x   = UniversalEmbeddingLayer()(inp)\n",
    "    x   = layers.Dense(256, activation=\"relu\", name=\"text_proj\")(x)\n",
    "    return Model(inputs=inp, outputs=x, name=\"TextBranch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fusion_model(embed_dim=128):\n",
    "    img_in = Input(shape=(256,), name=\"image_features\")\n",
    "    txt_in = Input(shape=(256,), name=\"text_features\")\n",
    "    merged = layers.Concatenate()([img_in, txt_in])\n",
    "    x = layers.Dense(embed_dim)(merged)\n",
    "    x = layers.Lambda(\n",
    "    lambda y: tf.math.l2_normalize(y, axis=1),\n",
    "    output_shape=lambda s: s\n",
    "    )(merged)\n",
    "\n",
    "    return Model([img_in, txt_in], x, name=\"FusionModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = build_image_branch()\n",
    "text_model = build_text_branch()\n",
    "fusion_model = build_fusion_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Triplet loss and optimizer\n",
    "def triplet_loss(a,p,n, margin=0.3):\n",
    "    dp = tf.reduce_sum(tf.square(a-p), axis=1)\n",
    "    dn = tf.reduce_sum(tf.square(a-n), axis=1)\n",
    "    return tf.reduce_mean(tf.maximum(dp - dn + margin, 0.0))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pr/59yr414s4tv0d5trcpxhsvc80000gp/T/ipykernel_25531/4113443341.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mp_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfusion_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mn_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfusion_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mni\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplet_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         grads = tape.gradient(\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtext_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               \u001b[0moutput_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutput_gradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m\"Unknown value for unconnected_gradients: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0munconnected_gradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m           \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[1;32m    595\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1495\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 7. Training loop with early stopping + accuracy\n",
    "EPOCHS, PATIENCE = 5,2\n",
    "\n",
    "batch_size   = 32\n",
    "steps = (len(triplets) + batch_size - 1) // batch_size\n",
    "best_loss, no_improve = np.inf, 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss, total_acc, count = 0.0, 0, 0\n",
    "    print(f\"\\nüîÅ Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for i, (ai, at, pi, pt, ni, nt) in tqdm(\n",
    "            enumerate(ds), total=steps, desc=\"Training\", leave=False):\n",
    "        if i >= steps: break\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            a_emb = fusion_model([image_model(ai), text_model(at)], training=True)\n",
    "            p_emb = fusion_model([image_model(pi), text_model(pt)], training=True)\n",
    "            n_emb = fusion_model([image_model(ni), text_model(nt)], training=True)\n",
    "            loss = triplet_loss(a_emb, p_emb, n_emb)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            loss,\n",
    "            image_model.trainable_weights +\n",
    "            text_model.trainable_weights +\n",
    "            fusion_model.trainable_weights\n",
    "        )\n",
    "        optimizer.apply_gradients(zip(\n",
    "            grads,\n",
    "            image_model.trainable_weights +\n",
    "            text_model.trainable_weights +\n",
    "            fusion_model.trainable_weights\n",
    "        ))\n",
    "\n",
    "        # accumulate loss\n",
    "        total_loss += loss.numpy()\n",
    "\n",
    "        # compute ‚Äúaccuracy‚Äù: +1 if d(a,p) < d(a,n)\n",
    "        dp = np.sum((a_emb - p_emb)**2, axis=1)\n",
    "        dn = np.sum((a_emb - n_emb)**2, axis=1)\n",
    "        total_acc += np.mean(dp < dn)\n",
    "        count += 1\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    avg_acc  = total_acc / count\n",
    "    print(f\"‚úÖ Epoch {epoch+1}: Loss={avg_loss:.4f}, Acc={avg_acc:.4f}\")\n",
    "\n",
    "    # early stopping & saving\n",
    "    if avg_loss < best_loss - 1e-4:\n",
    "        best_loss, no_improve = avg_loss, 0\n",
    "        fusion_model.save(\"trained_mod/fusion_model.keras\")\n",
    "        image_model.save(\"trained_mod/image_model.keras\")\n",
    "        text_model.save(\"trained_mod/text_model.keras\")\n",
    "        print(\"üíæ Models saved.\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement for {no_improve} epoch(s).\")\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"üõë Early stopping at epoch {epoch+1}.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime, tensorflow as tf\n",
    "\n",
    "# create an output directory dated for easy tracking\n",
    "stamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "out_dir = f\"trained_models_{stamp}\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüíæ  Saving models to \\\"{out_dir}\\\" ‚Ä¶\")\n",
    "\n",
    "# 1Ô∏è‚É£ ‚Äì save COMPLETE models  (preferred for re-loading later)\n",
    "try:\n",
    "    image_model.save(f\"{out_dir}/image_model.keras\")\n",
    "    text_model.save(f\"{out_dir}/text_model.keras\")\n",
    "    fusion_model.save(f\"{out_dir}/fusion_model.keras\")\n",
    "    print(\"‚úÖ  Full .keras model files saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Saving full models failed: {e}\")\n",
    "\n",
    "# 2Ô∏è‚É£ ‚Äì save WEIGHTS ONLY  (fallback / lighter checkpoint)\n",
    "try:\n",
    "    image_model.save_weights(f\"{out_dir}/image_model.weights.h5\")\n",
    "    text_model.save_weights(f\"{out_dir}/text_model.weights.h5\")\n",
    "    fusion_model.save_weights(f\"{out_dir}/fusion_model.weights.h5\")\n",
    "    print(\"‚úÖ  Weights (.weights.h5) saved.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Saving weights failed: {e}\")\n",
    "\n",
    "print(\"üéâ  All save operations complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Recommendation pipeline using keyword-based embeddings\n",
    "def get_embedding(path, sentence):\n",
    "    img = load_img(path, target_size=(224,224), color_mode='rgb')\n",
    "    img = img_to_array(img); img = tf.keras.applications.resnet50.preprocess_input(img)\n",
    "    img = np.expand_dims(img,0)\n",
    "    ie = image_model.predict(img, verbose=0)[0]\n",
    "    te = use([sentence])[0].numpy()\n",
    "    return np.concatenate([ie, te])\n",
    "\n",
    "def recommend(query_img, query_caption, catalog, top_k=5):\n",
    "    q_emb = get_embedding(query_img, query_caption)\n",
    "    scores = [(p, cosine_similarity([q_emb],[get_embedding(p,c)])[0][0]) for p,c in catalog.items()]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    os.makedirs(\"trained_mod\", exist_ok=True)\n",
    "    # Build a catalog of first 50 images with keyword sentences\n",
    "    catalog = {\n",
    "        f\"flickr8k/images/{img}\": keyword_sentences[img]\n",
    "        for img in list(image_captions.keys())[:50]\n",
    "    }\n",
    "    query_img = list(catalog.keys())[0]\n",
    "    query_kw  = catalog[query_img]\n",
    "    top6 = recommend(query_img, query_kw, catalog, top_k=6)\n",
    "\n",
    "    print(\"\\nüéØ Top Recommendations:\")\n",
    "    for p,s in top6:\n",
    "        print(f\"{p}: {s:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
